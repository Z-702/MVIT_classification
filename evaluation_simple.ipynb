{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "import vtk\n",
    "import h5py\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_curve, precision_score, recall_score, f1_score,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame()\n",
    "stride = 1\n",
    "threshold = 0.5\n",
    "for m in range(4,9):\n",
    "    fold = foldraw[foldraw['age']//stride==m]\n",
    "    # fold = fold[fold['fold']==0]\n",
    "    print(f'Age:[{stride*m},{stride*(m+1)}),Number: {len(fold)}, Positive: {(fold[\"labels\"]==1).sum()}')\n",
    "    labels = fold['labels'].values\n",
    "    preds = fold['preds'].values\n",
    "    fpr,tpr,threshold = roc_curve(labels,preds)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr,tpr)\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve of Age:[{},{})'.format(stride*m,stride*(m+1)))\n",
    "    plt.savefig(f'roc_{m}.png',dpi=300)\n",
    "    threshold = 0.5\n",
    "#     print('Threshold:',threshold)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels,preds>threshold).ravel()\n",
    "    print('Accuracy: {:.3f}'.format((tn+tp)/(tn+fp+fn+tp)))\n",
    "    print('Specificity: {:.3f}, Sensitivity: {:.3f}'.format(tn/(tn+fp),tp/(tp+fn)))\n",
    "    print('AUC: {:.3f}'.format(roc_auc_score(labels,preds)))\n",
    "#     for threshold in np.linspace(0,1,40):\n",
    "        \n",
    "\n",
    "#     predsi = preds>0.5\n",
    "#     mixed = [((labels==0)&(predsi==labels)).sum(),\n",
    "#             ((labels==0)&(predsi!=labels)).sum(),\n",
    "#             ((labels==1)&(predsi==labels)).sum(),\n",
    "#             ((labels==1)&(predsi!=labels)).sum()]\n",
    "#     prec = mixed[0]/(mixed[0]+mixed[1]+1e-6)\n",
    "#     recall = mixed[0]/(mixed[0]+mixed[3]+1e-6)\n",
    "#     f1 = 2*prec*recall/(prec+recall+1e-6)\n",
    "#     print(f'Precision: {prec:.2f}, Recall: {recall:.2f}, AUC: {roc_auc_score(labels,preds):.2f}, Accuracy: {(mixed[0]+mixed[2])/sum(mixed):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.read_csv('./fold.csv')['DX_GROUP']==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nib.load('/data04/301_postprocess/301_embed/tractoembedding/sub-14117/tractoembedding/da-full/sub-14117-trace2_CLR_sz320.nii.gz').get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_csv('sub_info.csv')\n",
    "\n",
    "demo_raw = pd.read_csv('new_400_301_fine_useful.csv')\n",
    "\n",
    "# add age from demo to demo raw respect to SUB_ID\n",
    "\n",
    "demo_raw['age'] = np.nan\n",
    "demo_raw['sex'] = np.nan\n",
    "for i in range(len(demo_raw)):\n",
    "    demo_raw.loc[i, 'age'] = demo[demo['SUB_ID'] == demo_raw.loc[i, 'SUB_ID']]['age'].values[0]\n",
    "    demo_raw.loc[i,'sex'] = demo[demo['SUB_ID'] == demo_raw.loc[i, 'SUB_ID']]['sex'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_raw1 = demo_raw[demo_raw['age']<11]\n",
    "demo_raw1['age_group'] = np.round(demo_raw1['age']) // 2 \n",
    "demo_raw1.dropna(axis=0,inplace=True)\n",
    "\n",
    "demo_raw1['final_group'] = demo_raw1['age_group']*10 + demo_raw1['sex']\n",
    "for i in np.unique(demo_raw1['final_group']):\n",
    "    # print(i, len(demo_raw1[demo_raw1['final_group'] == i]))\n",
    "    demo_raw1[demo_raw1['final_group']==i]\n",
    "    sub_id = demo_raw1.loc[demo_raw1['final_group']==i,'SUB_ID'].to_numpy()\n",
    "\n",
    "    np.random.shuffle(sub_id)\n",
    "    \n",
    "    for i in range(5):\n",
    "        group_list = sub_id[i*len(sub_id)//5:(i+1)*len(sub_id)//5]\n",
    "        print(i, group_list)\n",
    "        demo_raw1.loc[demo_raw1['SUB_ID'].isin(group_list),'fold']=i\n",
    "        # demo_raw1.loc[demo_raw1['SUB_ID'].isin(group_list),'fold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for i in range(5):\n",
    "    i = float(i)\n",
    "    demo_raw1.loc[demo_raw1['fold'] == i,'fold_new'] = 1\n",
    "    demo_raw1.loc[demo_raw1['fold'] != i,'fold_new'] = 0\n",
    "        # demo_raw1.loc[data_5_fold[i],'fold'] = i\n",
    "    # for i, fold in enumerate(data_5_fold):\n",
    "    #     if i == 2:\n",
    "    #         demo_raw1.loc[fold,'fold'] = 1\n",
    "    #     else:\n",
    "    #         demo_raw1.loc[fold,'fold'] = 0\n",
    "    #     print(demo_raw1.loc[fold, 'age'].mean())\n",
    "    #     print(demo_raw1.loc[fold, 'age'].std())\n",
    "    # sns.histplot(demo_raw['age'], bins=20)\n",
    "    # demo_raw1 = demo_raw[demo_raw['age']<14]\n",
    "    fig = sns.displot(x='age', data=demo_raw1, kind='hist',col='fold_new', bins=40,hue='sex')\n",
    "    plt.savefig(f'dis_fig/age_distribution{i :.0f}.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_pos(pos,name):\n",
    "    if pos=='left':\n",
    "        if pos in name:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    if pos=='right':\n",
    "        if pos in name:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    if pos=='commisural':\n",
    "        if 'left' not in name and 'right' not in name:\n",
    "            return True\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vtk data\n",
    "filename='/data01/zixi/tractoembedding_1000/749361/tracts/749361.vtp'\n",
    "reader = vtk.vtkXMLPolyDataReader()\n",
    "reader.SetFileName(filename)\n",
    "reader.Update()\n",
    "outpd = reader.GetOutput()\n",
    "# print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 320\n",
    "mask_d = 320\n",
    "pos = 'left'\n",
    "from itertools import product\n",
    "for mask_d, pos in product([320],['left','right','commisural']):\n",
    "    embed = np.load('/data01/zixi/tractoembedding_1000/749361/embed_feat.npy')\n",
    "    mask = np.load(f'attns/precision_sz{mask_d}_mode_{pos}_total.npy').squeeze()\n",
    "    f = h5py.File('/data01/zixi/tractoembedding_1000/749361/tracts/749361.h5', \"r\")\n",
    "    tract_list = f['tract_list'][:]\n",
    "    tract_name = f['tract_name'][:]\n",
    "    tracts_id = np.concatenate([np.where(tract_list==i)[0] for i,name in enumerate(tract_name) if if_pos(pos,str(name))])\n",
    "    print(tracts_id)\n",
    "    for name in np.unique(tract_name[tract_list[tracts_id].astype(int)]):\n",
    "        write_vtp(outpd,tracts_id,str(name))\n",
    "\n",
    "#     embed = embed[tracts_id,:]\n",
    "#     embedding_D = embed*50\n",
    "#     dx = embedding_D[:, 0].round() + D / 2 - D / 10\n",
    "#     dy = embedding_D[:, 1].round() + D / 2 - D / 10\n",
    "#     plt.figure(figsize=(5,5))\n",
    "#     plt.scatter(dx, dy)\n",
    "#     plt.xlim(0, D)\n",
    "#     plt.ylim(D, 0)\n",
    "#     mean,std = mask.mean(),mask.std()\n",
    "#     zscore = (mask-mean)/std\n",
    "#     pvals = 2*(1-norm.cdf(np.abs(zscore)))\n",
    "#     pvals_flat = pvals.flatten()\n",
    "#     sign_mask, pvals_corrected,_,_ = multipletests(pvals_flat, alpha=0.01, method='bonferroni')\n",
    "#     pvals_corrected = pvals_corrected.reshape(pvals.shape)\n",
    "#     sign_mask = sign_mask.reshape(pvals.shape)\n",
    "#     mask  = sign_mask\n",
    "#     x,y = np.where(mask)\n",
    "#     index = np.where(np.bitwise_and(dx[...,None]==x[None,...],dy[...,None]==y[None,...,]).sum(1).astype(bool))\n",
    "#     embed_masked = embed[index,:]\n",
    "#     plt.scatter(dx[index], dy[index])\n",
    "#     plt.title(f'{pos}, Resolusion:{mask_d}')\n",
    "#     # plt.figure(figsize=(5,5))\n",
    "#     plt.imshow(mask.T,alpha=1,cmap='gray')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.savefig(f'attns_fig/precision_sz{mask_d}_mode_{pos}_total.png',dpi=300)\n",
    "#     # plt.figure(figsize=(5,5))\n",
    "# # plt.imshow(zscore.T,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_vtp(outpd,mask_id,name='tract'):\n",
    "    path = pathlib.Path(f'tracts')\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    writer = vtk.vtkXMLPolyDataWriter()\n",
    "    writer.SetDataModeToBinary()\n",
    "    outlines = vtk.vtkCellArray()\n",
    "    outpoints = vtk.vtkPoints()\n",
    "    polydata = vtk.vtkPolyData()\n",
    "    outlines.InitTraversal()\n",
    "    for i in mask_id.squeeze():\n",
    "        # if 'T_Sup_left' in str(tract_name[tract_list[i].astype(int)]):\n",
    "        if str(tract_name[tract_list[i].astype(int)]) == str(name):\n",
    "            cellptids = vtk.vtkIdList()\n",
    "            for j in range(outpd.GetCell(i).GetNumberOfPoints()):\n",
    "                idx = outpoints.InsertNextPoint(outpd.GetPoint(outpd.GetCell(i).GetPointId(j)))\n",
    "                cellptids.InsertNextId(idx)\n",
    "            outlines.InsertNextCell(cellptids)\n",
    "        \n",
    "    polydata.SetLines(outlines)\n",
    "    polydata.SetPoints(outpoints)\n",
    "    writer.SetInputData(polydata)\n",
    "    name = name.split('\\'')[1]\n",
    "    writer.SetFileName(path/(name+'.vtp'))\n",
    "    writer.Update()\n",
    "for name in np.unique(tract_name[tract_list[tracts_id].astype(int)]):\n",
    "    write_vtp(outpd,tracts_id,str(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(2,2,1)\n",
    "ax.imshow(data[...,0])\n",
    "ax = plt.subplot(2,2,2)\n",
    "ax.imshow(data[...,1])\n",
    "ax = plt.subplot(2,2,3)\n",
    "ax.imshow(data[...,2])\n",
    "ax = plt.subplot(2,2,4)\n",
    "ax.imshow(data.sum(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "fold = pd.read_csv('fold.csv')\n",
    "def extract_acc(path='/data04/junyi/results/mvit_nonorm_balanced_fold'):\n",
    "    a = {}\n",
    "    aug_num_lst = []\n",
    "    for path in sorted(Path(path).glob('**/*.log')):\n",
    "        pattern = re.findall(r\"[^_]+\",path.parent.name)\n",
    "        # print(pattern)\n",
    "        aug_num = int(pattern[-2])\n",
    "        # print(path.parent.name.rsplit('_'))\n",
    "        with open(path,mode='r') as f:\n",
    "            for line in f:\n",
    "                if ('The Final Min Err of fold') in line:\n",
    "            \n",
    "                    fold_num,acc = line.strip().rsplit('The Final Min Err of fold')[-1].split(':')\n",
    "                    num = (fold['fold'] == int(fold_num)).sum()\n",
    "                    print(num,fold_num)\n",
    "                    if int(fold_num) in a:\n",
    "                        a[int(fold_num)].append(float(acc)*num/len(fold))\n",
    "                    else:\n",
    "                        a[int(fold_num)] = [float(acc)*num/len(fold)]\n",
    "                    break\n",
    "        aug_num_lst.append(aug_num)\n",
    "    res = []\n",
    "    [res.append(x) for x in aug_num_lst if x not in res]\n",
    "    df = pd.DataFrame(a)\n",
    "    df.index = [f'Aug:{i}'for i in res]\n",
    "    df.sort_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in sorted(Path('/data06/junyi/results').iterdir()):\n",
    "    try:\n",
    "        if path.is_dir():\n",
    "            df = extract_acc(path)\n",
    "            print(f'Name:{path.name}\\nACC:\\n{100-df.sum(axis=1)}')\n",
    "    except:\n",
    "        print(f'Name:{path.name}\\nACC:Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "df = pd.read_csv('/data01/zixi/TractoFormer/TractoFormer-MVIT-main/HCP_1000.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1920764/1743459474.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mvit.datasets.tractoembedding.Tractoembedding'>\n",
      "(100, 3)\n",
      "å½“å‰ batch çš„è„‘ ID: tensor([196851, 130720, 175035, 158136])\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,3\n",
    "# %env LD_LIBRARY_PATH = ''\n",
    "try:\n",
    "  from typing import Literal\n",
    "  ModelName = Literal[\"MViTv2-B\", \"MViTv2-S\", \"MViTv2-L\", \"MViTv2-T\", ]\n",
    "except ImportError:\n",
    "  ModelName = str\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "from mvit.datasets.tractoembedding import Tractoembedding\n",
    "\n",
    "gpus_str = ''\n",
    "gpus_list = []\n",
    "# for i in range(torch.cuda.device_count()):\n",
    "#     used,all=torch.cuda.mem_get_info(device=i)\n",
    "#     if used/all >=0.80:\n",
    "#         gpus_str+=f'{i},'\n",
    "#         gpus_list += [i]\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = gpus_str[:-1]\n",
    "# world_size = num_proc * num_shards\n",
    "# rank = shard_id * num_proc + local_rank\n",
    "from matplotlib import pyplot as plt\n",
    "def load_model_and_preproc(model=\"MViTv2-mri\", use_cuda:bool=torch.cuda.is_available(), use_half:bool=True):\n",
    "    \"\"\"\n",
    "    Load an MViT2 model and get the image transformer\n",
    "    :param model: the name of the model. Should be  one of \"MViTv2-B\", \"MViTv2-S\", \"MViTv2-L\", \"MViTv2-T\". Defaults to \"MViTv2-B\".\n",
    "    :param use_cuda: whether to use cuda. Defaults to True if cuda is available.\n",
    "    :param use_half: whether to use half-precision. Defaults to True, but will not be used if cuda is not used.\n",
    "    :returns a tuple of (the model, image transformer)\n",
    "    \"\"\"\n",
    "    model = model.replace('-','_')\n",
    "    use_half &= use_cuda\n",
    "\n",
    "    import urllib.request\n",
    "    from mvit.config.defaults import get_cfg\n",
    "    from mvit.models import build_model\n",
    "    from mvit.datasets import Tractoembedding\n",
    "    from mvit.datasets import loader\n",
    "    mode = 'fusion'#å¤šæ¨¡æ€èåˆæ¨¡å¼ï¼ŒFA1,densityï¼Œtrace1\n",
    "    ckpt_path = '/data01/zixi/TractoFormer/TractoFormer-MVIT-main/synthetic_v1_0/checkpoints/checkpoint_epoch_00140.pyth'\n",
    "    # ckpt_path = f\"/data04/junyi/results/mvit_balanced_fold_new_norm_direct/{mode}_3_030_0123/checkpoints/checkpoint_epoch_00160.pyth\"\n",
    "    # if not os.path.exists(ckpt_path):\n",
    "    #     urllib.request.urlretrieve(f\"https://dl.fbaipublicfiles.com/mvit/mvitv2_models/{ckpt_path}\", ckpt_path)\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # cfg.MVIT.CLS_EMBED_ON = True\n",
    "    cfg.DATA.PATH_TO_DATA_DIR = '/data01/zixi/synthetic_noise/selected_100_males.csv'\n",
    "    # cfg.merge_from_file(f\"./configs/test/{model.replace('ViT','VIT')}_test.yaml\")\n",
    "    cfg.merge_from_file('configs/MVITv2_mri.yaml')\n",
    "    cfg.DATA.MODE = 'fusion'\n",
    "    cfg.MVIT.CLS_EMBED_ON = True\n",
    "    cfg.DATA_AUG_NUM =1\n",
    "    cfg.DATA_NUM = 3\n",
    "    cfg.NUM_GPUS = 1\n",
    "    cfg.DATA_MODE = mode\n",
    "    cfg.TRAIN.BATCH_SIZE = 4\n",
    "\n",
    "    model = build_model(cfg)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    \n",
    "    model.eval()\n",
    "    # if use_half:\n",
    "    #     model = model.half()\n",
    "    return model,loader.construct_loader(cfg, \"val\",0)\n",
    "    \n",
    "model, datagen = load_model_and_preproc()\n",
    "datagen.dataset.return_subid = True\n",
    "input_images, labels, sub_ids = next(iter(datagen))\n",
    "print(\"å½“å‰ batch çš„è„‘ ID:\", sub_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1920764/4015468948.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('/data01/zixi/TractoFormer/TractoFormer-MVIT-main/synthetic_v1_0/checkpoints/checkpoint_epoch_00140.pyth', map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_list.0.blocks.0.norm1.weight', 'model_list.0.blocks.0.norm1.bias', 'model_list.0.blocks.0.attn.rel_pos_h', 'model_list.0.blocks.0.attn.rel_pos_w', 'model_list.0.blocks.0.attn.qkv.weight', 'model_list.0.blocks.0.attn.qkv.bias', 'model_list.0.blocks.0.attn.proj.weight', 'model_list.0.blocks.0.attn.proj.bias', 'model_list.0.blocks.0.attn.pool_q.weight', 'model_list.0.blocks.0.attn.norm_q.weight', 'model_list.0.blocks.0.attn.norm_q.bias', 'model_list.0.blocks.0.attn.pool_k.weight', 'model_list.0.blocks.0.attn.norm_k.weight', 'model_list.0.blocks.0.attn.norm_k.bias', 'model_list.0.blocks.0.attn.pool_v.weight', 'model_list.0.blocks.0.attn.norm_v.weight', 'model_list.0.blocks.0.attn.norm_v.bias', 'model_list.0.blocks.0.norm2.weight', 'model_list.0.blocks.0.norm2.bias', 'model_list.0.blocks.0.mlp.fc1.weight']\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load('/data01/zixi/TractoFormer/TractoFormer-MVIT-main/synthetic_v1_0/checkpoints/checkpoint_epoch_00140.pyth', map_location='cpu')\n",
    "print([k for k in ckpt[\"model_state\"].keys() if \"blocks\" in k][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_images, labels, sub_ids = next(iter(datagen))\n",
    "# print(\"å½“å‰ batch çš„è„‘ ID:\", sub_ids)\n",
    "# # input_images, labels= next(iter(datagen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_images,labels in datagen:\n",
    "#     img = input_images[0][0][0]\n",
    "#     mask = img!=0\n",
    "#     # mt = masked_tensor(img,img!=0)\n",
    "#     mean = img.mean((2,3),  keepdim=True).mean(0,keepdim=True)\n",
    "#     std = img.std((2,3),keepdim=True).mean(0,keepdim=True)\n",
    "#     img1 = ((img-mean)/std)\n",
    "#     # img1[~mask] = 0\n",
    "#     print(mean.squeeze(),std.squeeze())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch sees 2 GPU(s).\n",
      "Using device: 0 â†’ NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "allimages = []\n",
    "for images4model in input_images[0]:\n",
    "    imagecuda = [image.cuda() for image in images4model]\n",
    "    allimages.append(imagecuda)\n",
    "print(f\"PyTorch sees {torch.cuda.device_count()} GPU(s).\")\n",
    "print(f\"Using device: {torch.cuda.current_device()} â†’ {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(allimages)\n\u001b[0;32m----> 4\u001b[0m mixed \u001b[38;5;241m=\u001b[39m [((labels\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m&\u001b[39m(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mlabels\u001b[49m))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum(),\n\u001b[1;32m      5\u001b[0m                  ((labels\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m&\u001b[39m(preds\u001b[38;5;241m!=\u001b[39mlabels))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum(),\n\u001b[1;32m      6\u001b[0m                  ((labels\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m&\u001b[39m(preds\u001b[38;5;241m==\u001b[39mlabels))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum(),\n\u001b[1;32m      7\u001b[0m                  ((labels\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m&\u001b[39m(preds\u001b[38;5;241m!=\u001b[39mlabels))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()]\n\u001b[1;32m      8\u001b[0m prec \u001b[38;5;241m=\u001b[39m mixed[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m(mixed[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mmixed[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m recall \u001b[38;5;241m=\u001b[39m mixed[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m(mixed[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mmixed[\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():   \n",
    "    model.eval()\n",
    "    preds = model(allimages)\n",
    "mixed = [((labels==0)&(preds==labels)).float().sum(),\n",
    "                 ((labels==0)&(preds!=labels)).float().sum(),\n",
    "                 ((labels==1)&(preds==labels)).float().sum(),\n",
    "                 ((labels==1)&(preds!=labels)).float().sum()]\n",
    "prec = mixed[0]/(mixed[0]+mixed[1])\n",
    "recall = mixed[0]/(mixed[0]+mixed[3])\n",
    "f1 = 2*prec*recall/(prec+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #å åŠ åˆ°fa imageä¸Šé¢çš„\n",
    "# from itertools import product\n",
    "# import pathlib\n",
    "# import torch\n",
    "# from torchvision.transforms import functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "# import nibabel as nib\n",
    "\n",
    "# # ===================== åŸºç¡€å‚æ•° =====================\n",
    "# precison_list = ['sz80', 'sz160', 'sz320'][::-1]\n",
    "# mode_list = ['Left', 'Right', 'Commisural']\n",
    "\n",
    "# # è·å–é¢„æµ‹æ­£ç¡®çš„ sample mask\n",
    "# v, i = torch.topk(preds, 1)\n",
    "# labels = labels.cuda()\n",
    "# m = (labels == i.squeeze())\n",
    "# print(\"åŒ¹é…ç»“æœ m:\", m)\n",
    "\n",
    "# # ===================== è·å–çœŸå® sub_id =====================\n",
    "# sub_id = sub_ids[0].item() if torch.is_tensor(sub_ids[0]) else sub_ids[0]\n",
    "# root = f\"/data01/zixi/tractoembedding_PPMI_143/{sub_id}\"\n",
    "# print(f\"å½“å‰å¯è§†åŒ–çš„ subject: {sub_id}\")\n",
    "\n",
    "# # ===================== tract ä¿¡æ¯ï¼ˆè‡ªåŠ¨ï¼‰ =====================\n",
    "# h5_path = f\"{root}/tracts/{sub_id}.h5\"\n",
    "# with h5py.File(h5_path, \"r\") as f:\n",
    "#     tract_name = f['tract_name'][:]\n",
    "# print(\"Tract åç§°æ•°é‡:\", len(np.unique(tract_name)))\n",
    "\n",
    "# # ===================== å‚è€ƒå½±åƒï¼ˆFAï¼‰ =====================\n",
    "# fa_path = f\"{root}/tractoembedding/da-full/{sub_id}-FA1_CLR_sz640.nii.gz\"\n",
    "# fa_img = nib.load(fa_path)\n",
    "# fa_data = fa_img.get_fdata()\n",
    "# affine = fa_img.affine\n",
    "# inv_affine = np.linalg.inv(affine)\n",
    "# X, Y, Z = fa_data.shape\n",
    "\n",
    "# print(f\"[FA] ä½“ç´ å½¢çŠ¶: {fa_data.shape}, ä»¿å°„:\\n{affine}\")\n",
    "\n",
    "# # å¯è§†åŒ–å¹³é¢ä¸åˆ‡ç‰‡\n",
    "# plane = \"axial\"\n",
    "# slice_idx = max(1, Z // 2)\n",
    "\n",
    "\n",
    "# # ===================== Heatmap ä¸Šé‡‡æ · =====================\n",
    "# def overlay_on_slice(attn_20x20, volume, plane, slice_index):\n",
    "#     if plane == \"axial\":\n",
    "#         base_slice = volume[:, :, slice_index]\n",
    "#         target_hw = (base_slice.shape[1], base_slice.shape[0])\n",
    "#     else:\n",
    "#         raise ValueError(\"ä»…å®ç° axial æ¨¡å¼\")\n",
    "\n",
    "#     attn_t = torch.tensor(attn_20x20, dtype=torch.float32).unsqueeze(0)\n",
    "#     attn_rs = F.resize(attn_t, target_hw, F.InterpolationMode.BILINEAR)[0].numpy()\n",
    "#     return base_slice, attn_rs\n",
    "\n",
    "\n",
    "# # ===================== ç»˜å›¾ï¼ˆæ—  fiberï¼Œåª attentionï¼‰ =====================\n",
    "# def plot_slice(base_slice, attn_rs, title, save_path):\n",
    "#     fig, ax = plt.subplots(figsize=(7, 7))\n",
    "#     ax.imshow(base_slice.T, cmap='gray', alpha=0.75, origin='lower')\n",
    "#     ax.imshow(attn_rs.T, cmap='jet', alpha=0.35, origin='lower', vmin=0, vmax=0.05)\n",
    "\n",
    "#     ax.set_title(title, fontsize=12)\n",
    "#     ax.axis('off')\n",
    "#     fig.tight_layout()\n",
    "#     fig.savefig(save_path, dpi=300)\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# # ===================== ç”Ÿæˆå¯è§†åŒ– =====================\n",
    "# out_dir = pathlib.Path(\"./attns_only_attn\")\n",
    "# out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# for mi, pi in product(range(3), range(3)):\n",
    "#     attns = model.model_list[mi].blocks[2*pi].attn.attn_lst[m, :, 0, 1:].mean(0, keepdim=True)\n",
    "#     B, nH, nP = attns.shape\n",
    "#     for head in range(nH):\n",
    "#         attn_20 = attns[0, head].reshape(20, 20).cpu().numpy()\n",
    "#         base_slice, attn_rs = overlay_on_slice(attn_20, fa_data, plane, slice_idx)\n",
    "\n",
    "#         title = f\"Sub: {sub_id} | Prec: {precison_list[pi]}, Mode: {mode_list[mi]} | z={slice_idx}, Head={head}\"\n",
    "#         save_path = out_dir / f\"{sub_id}_prec_{precison_list[pi]}_mode_{mode_list[mi]}_{plane}_z{slice_idx}_head{head}.png\"\n",
    "\n",
    "#         plot_slice(base_slice, attn_rs, title, save_path)\n",
    "\n",
    "# print(\"ğŸ‰ å·²å®Œæˆï¼šçº¯ attention map å¯è§†åŒ–ï¼ˆæ—  fiberï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== æ£€æŸ¥è¾“å…¥æ–‡ä»¶ =====\n",
      "æ£€æŸ¥ /data01/zixi/synthetic_noise/synthetic_embedding/196851/tractoembedding_noisy/tractoembedding/da-full/196851-FA1_CLR_sz80.nii.gz\n",
      "  âœ” æ‰¾åˆ°æ–‡ä»¶\n",
      "æ£€æŸ¥ /data01/zixi/synthetic_noise/synthetic_embedding/196851/tractoembedding_noisy/tractoembedding/da-full/196851-FA1_CLR_sz160.nii.gz\n",
      "  âœ” æ‰¾åˆ°æ–‡ä»¶\n",
      "æ£€æŸ¥ /data01/zixi/synthetic_noise/synthetic_embedding/196851/tractoembedding_noisy/tractoembedding/da-full/196851-FA1_CLR_sz320.nii.gz\n",
      "  âœ” æ‰¾åˆ°æ–‡ä»¶\n",
      "\n",
      "ğŸ“Œ å¤„ç† subject 196851ï¼Œæ‰¾åˆ° 3 ä¸ªæœ‰æ•ˆè¾“å…¥æ–‡ä»¶\n",
      "\n",
      "\n",
      "====== å¤„ç†é€šé“ 0 ======\n",
      "80 é€šé“ 0 åŸå§‹ shape: (80, 80)\n",
      "80 é€šé“ 0 resize å shape: (160, 160)\n",
      "160 é€šé“ 0 åŸå§‹ shape: (160, 160)\n",
      "160 é€šé“ 0 resize å shape: (160, 160)\n",
      "320 é€šé“ 0 åŸå§‹ shape: (320, 320)\n",
      "320 é€šé“ 0 resize å shape: (160, 160)\n",
      "â­ é€šé“ 0 åˆå¹¶å®Œæˆ, mean å min=-0.07518885284662247, max=0.821058452129364\n",
      "\n",
      "====== å¤„ç†é€šé“ 1 ======\n",
      "80 é€šé“ 1 åŸå§‹ shape: (80, 80)\n",
      "80 é€šé“ 1 resize å shape: (160, 160)\n",
      "160 é€šé“ 1 åŸå§‹ shape: (160, 160)\n",
      "160 é€šé“ 1 resize å shape: (160, 160)\n",
      "320 é€šé“ 1 åŸå§‹ shape: (320, 320)\n",
      "320 é€šé“ 1 resize å shape: (160, 160)\n",
      "â­ é€šé“ 1 åˆå¹¶å®Œæˆ, mean å min=-0.18636031448841095, max=0.8340086340904236\n",
      "\n",
      "====== å¤„ç†é€šé“ 2 ======\n",
      "80 é€šé“ 2 åŸå§‹ shape: (80, 80)\n",
      "80 é€šé“ 2 resize å shape: (160, 160)\n",
      "160 é€šé“ 2 åŸå§‹ shape: (160, 160)\n",
      "160 é€šé“ 2 resize å shape: (160, 160)\n",
      "320 é€šé“ 2 åŸå§‹ shape: (320, 320)\n",
      "320 é€šé“ 2 resize å shape: (160, 160)\n",
      "â­ é€šé“ 2 åˆå¹¶å®Œæˆ, mean å min=-0.1992940902709961, max=0.7414708733558655\n",
      "\n",
      "ğŸ‰ FINISHED! æ–‡ä»¶ä¿å­˜äº:\n",
      "/data01/zixi/synthetic_noise/synthetic_embedding/196851/tractoembedding_noisy/tractoembedding/da-full/mean/196851-FA1_CLR_sz160_merged.nii.gz\n"
     ]
    }
   ],
   "source": [
    "#80*80så‡é‡‡æ ·åˆ°160*160ï¼Œ 320*320é™é‡‡æ ·åˆ°160*160\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------\n",
    "# å‡½æ•°ï¼šæŠŠå•é€šé“å›¾åƒ resize åˆ° 160Ã—160\n",
    "# ---------------------------\n",
    "def resize_to_160(img_2d):\n",
    "    t = torch.tensor(img_2d).unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    t_res = F.interpolate(\n",
    "        t, size=(160,160), mode=\"bilinear\", align_corners=False\n",
    "    )[0,0].numpy()\n",
    "    return t_res\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# å‡½æ•°ï¼šæŒ‰ C/L/R é€šé“åˆ†åˆ«ç»Ÿä¸€åˆ° 160Ã—160ï¼Œå¹¶æ±‚ mean\n",
    "# ---------------------------\n",
    "def merge_channels(sub_id):\n",
    "    root = f\"/data01/zixi/synthetic_noise/synthetic_embedding/{sub_id}/tractoembedding_noisy/tractoembedding/da-full\"\n",
    "\n",
    "    paths = {\n",
    "        \"80\":  f\"{root}/{sub_id}-FA1_CLR_sz80.nii.gz\",\n",
    "        \"160\": f\"{root}/{sub_id}-FA1_CLR_sz160.nii.gz\",\n",
    "        \"320\": f\"{root}/{sub_id}-FA1_CLR_sz320.nii.gz\",\n",
    "    }\n",
    "\n",
    "    print(\"===== æ£€æŸ¥è¾“å…¥æ–‡ä»¶ =====\")\n",
    "    available = []\n",
    "    for k, p in paths.items():\n",
    "        print(f\"æ£€æŸ¥ {p}\")\n",
    "        if os.path.exists(p):\n",
    "            print(\"  âœ” æ‰¾åˆ°æ–‡ä»¶\")\n",
    "            available.append((k, p))\n",
    "        else:\n",
    "            print(\"  âŒ æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "\n",
    "    if not available:\n",
    "        print(\"âŒ æ²¡æœ‰å¯ç”¨è¾“å…¥æ–‡ä»¶ï¼Œé€€å‡º\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nğŸ“Œ å¤„ç† subject {sub_id}ï¼Œæ‰¾åˆ° {len(available)} ä¸ªæœ‰æ•ˆè¾“å…¥æ–‡ä»¶\\n\")\n",
    "\n",
    "    # (160,160,3) â†’ [Comm, Left, Right]\n",
    "    merged = np.zeros((160,160,3), dtype=np.float32)\n",
    "\n",
    "    for ch in range(3):\n",
    "        print(f\"\\n====== å¤„ç†é€šé“ {ch} ======\")\n",
    "        resized_imgs = []\n",
    "\n",
    "        for size_label, file_path in available:\n",
    "            img = nib.load(file_path)\n",
    "            data = img.get_fdata().astype(np.float32)\n",
    "\n",
    "            channel_img = data[:, :, ch]\n",
    "            print(f\"{size_label} é€šé“ {ch} åŸå§‹ shape: {channel_img.shape}\")\n",
    "\n",
    "            resized = resize_to_160(channel_img)\n",
    "            print(f\"{size_label} é€šé“ {ch} resize å shape: {resized.shape}\")\n",
    "\n",
    "            resized_imgs.append(resized)\n",
    "\n",
    "        merged[:,:,ch] = np.mean(resized_imgs, axis=0)\n",
    "        print(f\"â­ é€šé“ {ch} åˆå¹¶å®Œæˆ, mean å min={merged[:,:,ch].min()}, max={merged[:,:,ch].max()}\")\n",
    "\n",
    "    mean_dir = os.path.join(root, \"mean\")\n",
    "    os.makedirs(mean_dir, exist_ok=True)\n",
    "\n",
    "    out_path = os.path.join(mean_dir, f\"{sub_id}-FA1_CLR_sz160_merged.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(merged, np.eye(4)), out_path)\n",
    "\n",
    "    print(f\"\\nğŸ‰ FINISHED! æ–‡ä»¶ä¿å­˜äº:\")\n",
    "    print(out_path)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# å®é™…æ‰§è¡Œ\n",
    "# ---------------------------\n",
    "merge_channels(\"196851\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä½¿ç”¨thresholdåœˆè½®å»“\n",
    "from itertools import product\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from skimage import measure\n",
    "\n",
    "# ===================== åŸºç¡€å‚æ•° =====================\n",
    "mode_list = ['Left', 'Right', 'Commisural']\n",
    "resolutions = ['sz80', 'sz160', 'sz320']\n",
    "pi_map = {'sz80':2, 'sz160':1, 'sz320':0}\n",
    "\n",
    "# ===================== æ­£ç¡® sample =====================\n",
    "v, i = torch.topk(preds, 1)\n",
    "labels = labels.cuda()\n",
    "m = (labels == i.squeeze())\n",
    "\n",
    "# ===================== sub_id =====================\n",
    "sub_id = sub_ids[0].item() if torch.is_tensor(sub_ids[0]) else sub_ids[0]\n",
    "root = f\"/data01/zixi/synthetic_noise/synthetic_embedding/{sub_id}\"\n",
    "\n",
    "# ===================== èƒŒæ™¯ï¼ˆ160Ã—160Ã—3ï¼‰ =====================\n",
    "bg_path = f\"{root}/tractoembedding_noisy/tractoembedding/da-full/mean/{sub_id}-FA1_CLR_sz160_merged.nii.gz\"\n",
    "bg_data = nib.load(bg_path).get_fdata().astype(np.float32)\n",
    "\n",
    "H, W, C = bg_data.shape\n",
    "\n",
    "# ===================== resize 20â†’160 =====================\n",
    "def resize20_to_160(attn20):\n",
    "    t = torch.tensor(attn20).unsqueeze(0).unsqueeze(0)\n",
    "    return F.interpolate(t, size=(H,W), mode=\"bilinear\", align_corners=False)[0,0].numpy()\n",
    "\n",
    "\n",
    "# ===================== Hotspot å¯è§†åŒ–ï¼ˆP95 é˜ˆå€¼ + ä¸åŒé¢œè‰²ï¼‰ =====================\n",
    "def plot_hotspot(base, attn160, title, path, color='red', p=99):\n",
    "    \"\"\"\n",
    "    base    : (H,W) èƒŒæ™¯ FA å•é€šé“\n",
    "    attn160 : (H,W) attentionï¼ˆä¸Šé‡‡æ ·åï¼‰\n",
    "    color   : è½®å»“é¢œè‰² ('red','blue','green')\n",
    "    p       : ç™¾åˆ†ä½é˜ˆå€¼ (é»˜è®¤ 95)\n",
    "    \"\"\"\n",
    "    # 1. é˜ˆå€¼\n",
    "    #attn160 = attn160.copy()\n",
    "    #attn160[:, 140:160] = 0   # æŠŠé¡¶éƒ¨ 20 åˆ—å¹²æ‰\n",
    "\n",
    "    T = np.percentile(attn160, p)\n",
    "    print(f\"ä½¿ç”¨ P{p} é˜ˆå€¼ = {T:.5f}\")\n",
    "\n",
    "    mask = attn160 > T\n",
    "    mask_disp = mask.T\n",
    "\n",
    "    # 2. å¯»æ‰¾è½®å»“\n",
    "    contours = measure.find_contours(mask_disp.astype(float), 0.5)\n",
    "\n",
    "    # 3. ç»˜å›¾\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "    ax.imshow(base.T, cmap='gray', alpha=0.85, origin='lower')\n",
    "    ax.imshow(attn160.T, cmap='jet',  alpha=0.45, origin='lower')\n",
    "\n",
    "    for cnt in contours:\n",
    "        ax.plot(cnt[:, 1], cnt[:, 0], color=color, linewidth=2)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===================== è¾“å‡ºç›®å½• =====================\n",
    "out_dir = pathlib.Path(f\"{root}/attns_merged_hotspot_P99\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# ===================== ä¸»å¾ªç¯ =====================\n",
    "for mi in range(3):\n",
    "\n",
    "    nH = model.model_list[mi].blocks[0].attn.num_heads\n",
    "\n",
    "    for head in range(nH):\n",
    "\n",
    "        # ä¸‰åˆ†è¾¨ç‡ attention â†’ 160Ã—160\n",
    "        attn_160_list = []\n",
    "        for res in resolutions:\n",
    "            pi = pi_map[res]\n",
    "            att = model.model_list[mi].blocks[2*pi].attn.attn_lst[m, :, 0, 1:].mean(0)\n",
    "            att20 = att[head].reshape(20,20).cpu().numpy()\n",
    "            attn_160_list.append(resize20_to_160(att20))\n",
    "\n",
    "        # merge\n",
    "        attn_merge_160 = np.maximum.reduce(attn_160_list)\n",
    "        np.save(out_dir / f\"{sub_id}_MI{mi}_HEAD{head}_attn160.npy\", attn_merge_160)\n",
    "\n",
    "        # ä¸åŒèƒŒæ™¯å¯¹åº”ä¸åŒé¢œè‰²\n",
    "        color_map = {\n",
    "            \"Commisural\": \"red\",\n",
    "            \"Left\": \"blue\",\n",
    "            \"Right\": \"green\"\n",
    "        }\n",
    "\n",
    "        for bg_ch, bg_name in zip([0,1,2], [\"Commisural\",\"Left\",\"Right\"]):\n",
    "\n",
    "            base_slice = bg_data[:,:,bg_ch]\n",
    "\n",
    "            save_path = out_dir / f\"{sub_id}_MI{mi}_HEAD{head}_BG{bg_name}_hotP95.png\"\n",
    "            title = f\"{sub_id} | MI{mi} | Head{head} | BG={bg_name} | hotspot(P95)\"\n",
    "\n",
    "            plot_hotspot(\n",
    "                base_slice,\n",
    "                attn_merge_160,\n",
    "                title,\n",
    "                save_path,\n",
    "                color=color_map[bg_name],\n",
    "                p=99   # â† è¿™é‡Œæ”¹æˆ 95\n",
    "            )\n",
    "\n",
    "print(\"ğŸ‰ å®Œæˆï¼šHotspot (P99) å¯è§†åŒ–\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ä½¿ç”¨ sample index: 0 (æ˜¯å¦æ­£ç¡®: False)\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.02659\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.02659\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.02659\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.02420\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.02420\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.02420\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.00645\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.00645\n",
      "ä½¿ç”¨ P99 é˜ˆå€¼ = 0.00645\n",
      "ğŸ‰ å®Œæˆï¼šHotspot (P99) å¯è§†åŒ–ï¼ˆä¿®å¤ç‰ˆï¼‰\n"
     ]
    }
   ],
   "source": [
    "#ä½¿ç”¨thresholdåœˆè½®å»“\n",
    "from itertools import product\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from skimage import measure\n",
    "\n",
    "# ===================== åŸºç¡€å‚æ•° =====================\n",
    "mode_list = ['Left', 'Right', 'Commisural']\n",
    "resolutions = ['sz80', 'sz160', 'sz320']\n",
    "pi_map = {'sz80':2, 'sz160':1, 'sz320':0}\n",
    "\n",
    "# ===================== æ­£ç¡® sampleï¼ˆä¿®å¤ï¼šåªå–ä¸€ä¸ªæ ·æœ¬ï¼‰ =====================\n",
    "v, i = torch.topk(preds, 1)\n",
    "labels = labels.cuda()\n",
    "m = (labels == i.squeeze())\n",
    "\n",
    "good_idx = m.nonzero(as_tuple=True)[0]\n",
    "if len(good_idx) > 0:\n",
    "    idx = good_idx[0].item()     # å–ç¬¬ 1 ä¸ªé¢„æµ‹æ­£ç¡®çš„æ ·æœ¬\n",
    "else:\n",
    "    idx = 0                      # è‹¥å…¨é”™ï¼Œå°± fallback åˆ° batch ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "\n",
    "print(f\"ğŸ“Œ ä½¿ç”¨ sample index: {idx} (æ˜¯å¦æ­£ç¡®: {m[idx].item()})\")\n",
    "\n",
    "# ===================== sub_id =====================\n",
    "sub_id = sub_ids[0].item() if torch.is_tensor(sub_ids[0]) else sub_ids[0]\n",
    "root = f\"/data01/zixi/synthetic_noise/synthetic_embedding/{sub_id}\"\n",
    "\n",
    "# ===================== èƒŒæ™¯ï¼ˆ160Ã—160Ã—3ï¼‰ =====================\n",
    "bg_path = f\"{root}/tractoembedding_noisy/tractoembedding/da-full/mean/{sub_id}-FA1_CLR_sz160_merged.nii.gz\"\n",
    "bg_data = nib.load(bg_path).get_fdata().astype(np.float32)\n",
    "\n",
    "H, W, C = bg_data.shape\n",
    "\n",
    "# ===================== resize 20â†’160 =====================\n",
    "def resize20_to_160(attn20):\n",
    "    attn20 = np.nan_to_num(attn20, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    t = torch.tensor(attn20).unsqueeze(0).unsqueeze(0)\n",
    "    return F.interpolate(t, size=(H,W), mode=\"bilinear\", align_corners=False)[0,0].numpy()\n",
    "\n",
    "# ===================== Hotspot å¯è§†åŒ– =====================\n",
    "def plot_hotspot(base, attn160, title, path, color='red', p=99):\n",
    "\n",
    "    # å¤„ç† NaN\n",
    "    attn160 = np.nan_to_num(attn160, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # å…¨é›¶ç›´æ¥è·³è¿‡\n",
    "    if not np.any(attn160):\n",
    "        print(f\"âš ï¸ {title}: attention map å…¨ 0ï¼Œè·³è¿‡ç»˜åˆ¶\")\n",
    "        return\n",
    "\n",
    "    # P99 é˜ˆå€¼ï¼ˆnan-safeï¼‰\n",
    "    T = np.nanpercentile(attn160, p)\n",
    "    print(f\"ä½¿ç”¨ P{p} é˜ˆå€¼ = {T:.5f}\")\n",
    "\n",
    "    mask = attn160 > T\n",
    "    mask_disp = mask.T\n",
    "\n",
    "    # å¯»æ‰¾è½®å»“\n",
    "    contours = measure.find_contours(mask_disp.astype(float), 0.5)\n",
    "\n",
    "    # ç”»å›¾\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.imshow(base.T, cmap='gray', alpha=0.85, origin='lower')\n",
    "    ax.imshow(attn160.T, cmap='jet',  alpha=0.45, origin='lower')\n",
    "\n",
    "    for cnt in contours:\n",
    "        ax.plot(cnt[:, 1], cnt[:, 0], color=color, linewidth=2)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ===================== è¾“å‡ºç›®å½• =====================\n",
    "out_dir = pathlib.Path(f\"{root}/attns_merged_hotspot_P99\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ===================== ä¸»å¾ªç¯ =====================\n",
    "for mi in range(3):\n",
    "\n",
    "    nH = model.model_list[mi].blocks[0].attn.num_heads\n",
    "\n",
    "    for head in range(nH):\n",
    "\n",
    "        attn_160_list = []\n",
    "\n",
    "        # ä¸‰åˆ†è¾¨ç‡ attention â†’ 160Ã—160\n",
    "        for res in resolutions:\n",
    "            pi = pi_map[res]\n",
    "\n",
    "            # â­ ä¿®å¤ï¼šä»ç¬¬ idx ä¸ªæ ·æœ¬å– attentionï¼Œè€Œä¸æ˜¯ç”¨ m åš mask\n",
    "            att = model.model_list[mi].blocks[2*pi].attn.attn_lst[idx, :, 0, 1:]\n",
    "\n",
    "            att20 = att[head].reshape(20,20).cpu().numpy()\n",
    "            attn_160_list.append(resize20_to_160(att20))\n",
    "\n",
    "        # merge\n",
    "        attn_merge_160 = np.maximum.reduce(attn_160_list)\n",
    "        attn_merge_160 = np.nan_to_num(attn_merge_160, nan=0.0)\n",
    "\n",
    "        np.save(out_dir / f\"{sub_id}_MI{mi}_HEAD{head}_attn160.npy\", attn_merge_160)\n",
    "\n",
    "        color_map = {\"Commisural\": \"red\", \"Left\": \"blue\", \"Right\": \"green\"}\n",
    "\n",
    "        for bg_ch, bg_name in zip([0,1,2], [\"Commisural\",\"Left\",\"Right\"]):\n",
    "\n",
    "            base_slice = bg_data[:,:,bg_ch]\n",
    "\n",
    "            save_path = out_dir / f\"{sub_id}_MI{mi}_HEAD{head}_BG{bg_name}_hotP99.png\"\n",
    "            title = f\"{sub_id} | MI{mi} | Head{head} | BG={bg_name} | hotspot(P99)\"\n",
    "\n",
    "            plot_hotspot(\n",
    "                base_slice,\n",
    "                attn_merge_160,\n",
    "                title,\n",
    "                save_path,\n",
    "                color=color_map[bg_name],\n",
    "                p=99\n",
    "            )\n",
    "\n",
    "print(\"ğŸ‰ å®Œæˆï¼šHotspot (P99) å¯è§†åŒ–ï¼ˆä¿®å¤ç‰ˆï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç”Ÿæˆleft/right/commisural tract attention æ’å\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# ============================================\n",
    "# (1) embed.npy â†’ 20Ã—20 pixel  â† æ³¨æ„ï¼šä¸æ˜¯ 160 ï¼\n",
    "# ============================================\n",
    "def embed_to_pixel_20(embed):\n",
    "    \"\"\"\n",
    "    embed.npy shape = (N, 2)\n",
    "    æ³¨æ„ï¼šTractoEmbed çš„ embed çœŸå®èŒƒå›´çº¦ [-2.5, 3.2]\n",
    "    å¯¹ 20Ã—20 grid çš„æ˜ å°„å¦‚ä¸‹ï¼š\n",
    "        pixel = round( (embed - min) / (max - min) * 19 )\n",
    "    \"\"\"\n",
    "    x = embed[:,0]\n",
    "    y = embed[:,1]\n",
    "\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    ymin, ymax = y.min(), y.max()\n",
    "\n",
    "    # normalize to [0, 19]\n",
    "    px = np.round((x - xmin) / (xmax - xmin) * 19).astype(int)\n",
    "    py = np.round((y - ymin) / (ymax - ymin) * 19).astype(int)\n",
    "\n",
    "    px = np.clip(px, 0, 19)\n",
    "    py = np.clip(py, 0, 19)\n",
    "\n",
    "    return np.stack([px, py], axis=1)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# (2) tract å·¦/å³/èƒ¼èƒä½“ åˆ†åŒº\n",
    "# ============================================\n",
    "def compute_fiber_location(tract_list, tract_name):\n",
    "    N = tract_list.shape[0]\n",
    "    loc = np.zeros(N, dtype=np.int32)\n",
    "    max_tid = len(tract_name) - 1\n",
    "\n",
    "    # ---- convert to safe integer tract id ----\n",
    "    tract_ids = tract_list.astype(np.int32)   # float â†’ int\n",
    "\n",
    "    # ---- clamp invalid values ----\n",
    "    tract_ids[tract_ids < 0] = 0\n",
    "    tract_ids[tract_ids > max_tid] = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        tid = tract_ids[i]\n",
    "\n",
    "        tname = tract_name[tid]\n",
    "        tname = tname.decode() if isinstance(tname, bytes) else tname\n",
    "        tname = tname.lower()\n",
    "\n",
    "        if \"left\" in tname:\n",
    "            loc[i] = 1\n",
    "        elif \"right\" in tname:\n",
    "            loc[i] = 2\n",
    "        elif \"comm\" in tname:\n",
    "            loc[i] = 0\n",
    "        else:\n",
    "            loc[i] = 0\n",
    "\n",
    "    return loc\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# (3) fiber-level attention\n",
    "# ============================================\n",
    "def compute_fiber_attention(att20, fiber_pix):\n",
    "    \"\"\"\n",
    "    att20 : (20,20) numpy array\n",
    "    fiber_pix : (N,2)\n",
    "    \"\"\"\n",
    "    px = fiber_pix[:,0]\n",
    "    py = fiber_pix[:,1]\n",
    "    return att20[px, py]   # (N,)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# (4) æŒ‰ tract èšåˆ attention\n",
    "# ============================================\n",
    "def aggregate_by_tract(fiber_attn, tract_list, tract_name, fiber_loc, hemi_code):\n",
    "    \"\"\"\n",
    "    fiber_attn : æ¯æ¡ fiber çš„ attention (N,)\n",
    "    tract_list : N ä¸ªå±äº 0~57 çš„ tract idï¼ˆå¯èƒ½æœ‰éæ³•å€¼ï¼‰\n",
    "    tract_name : 58 ä¸ª tract åç§°\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(fiber_attn)\n",
    "    max_tid = len(tract_name) - 1\n",
    "\n",
    "    # ---- æœ¬åŠçƒ fiber ----\n",
    "    idx = np.where(fiber_loc == hemi_code)[0]\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "\n",
    "    att = fiber_attn[idx]\n",
    "    tids = tract_list[idx]\n",
    "\n",
    "    tract_scores = {}\n",
    "\n",
    "    for tid, score in zip(tids, att):\n",
    "\n",
    "        # ========== ğŸ›¡ å¼ºåˆ¶å®‰å…¨è¿‡æ»¤ ==========\n",
    "        if np.isnan(tid):\n",
    "            tid = 0\n",
    "        else:\n",
    "            tid = int(tid)\n",
    "\n",
    "        if tid < 0 or tid > max_tid:\n",
    "            tid = 0\n",
    "        # ====================================\n",
    "\n",
    "        if tid not in tract_scores:\n",
    "            tract_scores[tid] = []\n",
    "        tract_scores[tid].append(score)\n",
    "\n",
    "    # ---- èšåˆå‡å€¼å¹¶è¯»å– tract_name ----\n",
    "    result = []\n",
    "    for tid, scores in tract_scores.items():\n",
    "        name = tract_name[tid]\n",
    "        name = name.decode() if isinstance(name, bytes) else name\n",
    "        result.append((name, np.mean(scores)))\n",
    "\n",
    "    # attention ä»å¤§åˆ°å°æ’åº\n",
    "    return sorted(result, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# (5) ä¸»æµç¨‹ï¼šçœŸæ­£ fiber-level attention åˆ†æ\n",
    "# ============================================\n",
    "def run_fiber_level(sub_id):\n",
    "\n",
    "    root = f\"/data01/zixi/tractoembedding_PPMI_143/{sub_id}\"\n",
    "\n",
    "    # è¯»å– embed\n",
    "    embed = np.load(f\"{root}/embed.npy\")\n",
    "    fiber_pix20 = embed_to_pixel_20(embed)\n",
    "\n",
    "    # tract ä¿¡æ¯\n",
    "    with h5py.File(f\"{root}/tracts/{sub_id}.h5\", \"r\") as f:\n",
    "        tract_list = f[\"tract_list\"][:]\n",
    "        tract_name = f[\"tract_name\"][:]\n",
    "\n",
    "    fiber_loc = compute_fiber_location(tract_list, tract_name)\n",
    "\n",
    "    # è¯»å–ä¸‰ä¸ª MI å¯¹åº”çš„ attention ï¼ˆ20Ã—20 åŸå›¾ï¼‰\n",
    "    att_files = {\n",
    "        \"Left\":       f\"{root}/attns_merged_hotspot_P99/{sub_id}_MI0_HEAD0_attn160.npy\",\n",
    "        \"Right\":      f\"{root}/attns_merged_hotspot_P99/{sub_id}_MI1_HEAD0_attn160.npy\",\n",
    "        \"Commisural\": f\"{root}/attns_merged_hotspot_P99/{sub_id}_MI2_HEAD0_attn160.npy\",\n",
    "    }\n",
    "\n",
    "    out_dir = f\"{root}/fiber_results_fiberLevel\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for region, fpath in att_files.items():\n",
    "\n",
    "        # --- åŸå§‹ att20 è·å– ---\n",
    "        # ä½ ä¿å­˜çš„æ˜¯ 160ï¼Œä½†æˆ‘ä»¬ä»æ¨¡å‹ç›´æ¥å– 20Ã—20 æ›´å¥½\n",
    "        # ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œä» 160 é™é‡‡æ ·å› 20\n",
    "        att160 = np.load(fpath)\n",
    "        att20 = att160.reshape(20,8,20,8).mean(axis=(1,3))  # block average to 20Ã—20\n",
    "\n",
    "        fiber_att = compute_fiber_attention(att20, fiber_pix20)\n",
    "\n",
    "        hemi = {\"Left\":1, \"Right\":2, \"Commisural\":0}[region]\n",
    "\n",
    "        tracts_rank = aggregate_by_tract(\n",
    "            fiber_att, tract_list, tract_name, fiber_loc, hemi\n",
    "        )\n",
    "\n",
    "        # å†™å…¥ txt\n",
    "        txt = f\"{out_dir}/{sub_id}_{region}.txt\"\n",
    "        with open(txt, \"w\") as f:\n",
    "            for name, score in tracts_rank:\n",
    "                f.write(f\"{score:.6f}  {name}\\n\")\n",
    "\n",
    "        print(f\"âœ” å†™å…¥ï¼š{txt}\")\n",
    "\n",
    "    print(\"\\nğŸ‰ fiber-level attention å®Œæˆï¼\")\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# æ‰§è¡Œ\n",
    "# --------------------------------------------\n",
    "run_fiber_level(\"129634\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç›´æ¥è¿›è¡Œmulti-scaleçš„ç‰ˆæœ¬\n",
    "from itertools import product\n",
    "import pathlib\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precison_list = ['sz80', 'sz160', 'sz320',]\n",
    "precison_list.reverse()\n",
    "mode_list = ['Left', 'Right', 'Commisural']\n",
    "\n",
    "v, i = torch.topk(preds, 1)\n",
    "labels = labels.cuda()\n",
    "m = labels == i.squeeze()\n",
    "print(m)\n",
    "\n",
    "\n",
    "import h5py\n",
    "\n",
    "f = h5py.File('/data01/zixi/synthetic_noise/synthetic_embedding/196851/tractoembedding_noisy/tracts/196851.h5', \"r\")\n",
    "tract_list = f['tract_list'][:]\n",
    "tract_name = f['tract_name'][:]\n",
    "print(len(np.unique(tract_name)))\n",
    "\n",
    "# âœ… åªåŠ è¿™ä¸€è¡Œï¼Œå‡è®¾ä½ æœ‰ sub_ids å˜é‡\n",
    "sub_id = sub_ids[0].item() if torch.is_tensor(sub_ids[0]) else sub_ids[0]\n",
    "\n",
    "for mi, pi in product(range(3), range(3)):\n",
    "    attns = model.model_list[mi].blocks[2*pi].attn.attn_lst[m, :, 0, 1:].mean(0, keepdim=True)\n",
    "    B, nH, nP = attns.shape\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    for head in range(nH):\n",
    "        attn = attns[0, head].reshape(-1, 20, 20).cpu()\n",
    "        mask = F.resize(attn, input_images[0][0][0].shape[-2:], F.InterpolationMode.BILINEAR)\n",
    "        # ä¿å­˜ä¸º .npy æ–‡ä»¶\n",
    "        npy_path = f'./attns/{sub_id}_precision_{precison_list[pi]}_mode_{mode_list[mi]}_head{head}.npy'\n",
    "        np.save(npy_path, mask.numpy())\n",
    "        \n",
    "        embed_feat = np.load('/data01/zixi/synthetic_noise/synthetic_embedding/196851/tractoembedding_noisy/embed_feat.npy')\n",
    "        embed = np.load('/data01/zixi/synthetic_noise/synthetic_embedding/196851/tractoembedding_noisy/embed.npy')\n",
    "        \n",
    "        print(embed.shape, embed_feat.shape)\n",
    "\n",
    "        true_x, true_y = np.where(mask[0] > 0.05)\n",
    "        indice = true_x + true_y * mask.shape[-1]\n",
    "        \n",
    "        fiber_indices = np.load('/data01/zixi/synthetic_noise/synthetic_embedding/196851/tractoembedding_noisy/tractoembedding/da-full/fiber_indices.npy')\n",
    "        true_fiber_indices = fiber_indices[indice]\n",
    "        \n",
    "        print(np.unique(tract_name[tract_list[true_fiber_indices].astype(int)]))\n",
    "        \n",
    "        \n",
    "\n",
    "        ax = plt.subplot(1, nH, head + 1, frameon=False)\n",
    "        ax.imshow(mask[0], vmin=0, vmax=0.05, cmap='jet')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Head: {head}', fontsize=8)\n",
    "    \n",
    "    # âœ… åœ¨è¿™é‡Œæ ‡é¢˜ä¸­åŠ ä¸Š ID\n",
    "    fig.suptitle(\n",
    "        f'Subject ID: {sub_id} | Precision: {precison_list[pi]}, Mode: {mode_list[mi]}',\n",
    "        fontsize=10, fontweight='bold', y=0.95\n",
    "    )\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    pathlib.Path('./attns').mkdir(exist_ok=True)\n",
    "    fig.savefig(f'./attns/precision_{precison_list[pi]}_mode_{mode_list[mi]}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import nibabel as nib\n",
    "\n",
    "# ===================== sub_id =====================\n",
    "sub_id = int(sub_ids[0])\n",
    "root = f\"/data01/zixi/tractoembedding_PPMI_143/{sub_id}\"\n",
    "\n",
    "print(\"=== Checking files under:\", root, \" ===\")\n",
    "\n",
    "# ===================== fiber_indices =====================\n",
    "fiber_indices_path = f\"{root}/tractoembedding/da-full/fiber_indices.npy\"\n",
    "fiber_indices = np.load(fiber_indices_path)\n",
    "print(\"\\nLoaded fiber_indices:\", fiber_indices_path)\n",
    "\n",
    "# ===================== tract_list & tract_name =====================\n",
    "h5_path = f\"{root}/tracts/{sub_id}.h5\"\n",
    "with h5py.File(h5_path, \"r\") as f:\n",
    "    tract_list = f[\"tract_list\"][:]\n",
    "    tract_name = f[\"tract_name\"][:]\n",
    "\n",
    "print(\"Loaded tract_list & tract_name:\", h5_path)\n",
    "\n",
    "# ===================== Debug è¾“å‡º =====================\n",
    "\n",
    "print(\"\\n========== DEBUG OUTPUT ==========\")\n",
    "print(\"fiber_indices shape =\", fiber_indices.shape)\n",
    "print(\"fiber_indices dtype =\", fiber_indices.dtype)\n",
    "\n",
    "print(\"min =\", fiber_indices.min(), \n",
    "      \" max =\", fiber_indices.max())\n",
    "\n",
    "print(\"\\ntract_list shape =\", tract_list.shape)\n",
    "print(\"tract_list dtype =\", tract_list.dtype)\n",
    "\n",
    "print(\"tract_name shape =\", tract_name.shape)\n",
    "\n",
    "# æ£€æŸ¥ tract_list æ˜¯å¦æœ‰éæ³• indexï¼ˆè¶…å‡º tract_name èŒƒå›´ï¼‰\n",
    "bad_tract_ids = tract_list[(tract_list < 0) | (tract_list >= len(tract_name))]\n",
    "\n",
    "print(\"\\néæ³• tract_id ä¸ªæ•° =\", len(bad_tract_ids))\n",
    "if len(bad_tract_ids) > 0:\n",
    "    print(\"å‰ 20 ä¸ªéæ³• tract_id:\", bad_tract_ids[:20])\n",
    "\n",
    "print(\"========== END DEBUG ==========\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import h5py\n",
    "import nibabel as nib\n",
    "\n",
    "# ===================== sub_id =====================\n",
    "sub_id = int(sub_ids[0])\n",
    "root = f\"/data01/zixi/tractoembedding_PPMI_143/{sub_id}\"\n",
    "\n",
    "# ===================== load fiber info =====================\n",
    "h5_path = f\"{root}/tracts/{sub_id}.h5\"\n",
    "with h5py.File(h5_path, \"r\") as f:\n",
    "    tract_list = f[\"tract_list\"][:]\n",
    "    tract_name = f[\"tract_name\"][:]\n",
    "\n",
    "fiber_indices = np.load(f\"{root}/tractoembedding/da-full/fiber_indices.npy\")\n",
    "\n",
    "# ===================== load one merged embedding to get size =====================\n",
    "emb_path = f\"{root}/tractoembedding/da-full/mean/{sub_id}-FA1_CLR_sz160_merged.nii.gz\"\n",
    "emb = nib.load(emb_path)\n",
    "H, W, _ = emb.shape\n",
    "\n",
    "# ===================== threshold =====================\n",
    "def threshold_p97(attn160):\n",
    "    vals = attn160[attn160 > 0]\n",
    "    if vals.size == 0:\n",
    "        return 999\n",
    "    return np.percentile(vals, 97)\n",
    "\n",
    "# ===================== hotspot â†’ fibers =====================\n",
    "def extract_fibers(attn160, T):\n",
    "\n",
    "    mask = attn160 > T\n",
    "    yy, xx = np.where(mask)\n",
    "    if len(xx) == 0:\n",
    "        return []\n",
    "\n",
    "    # 160Ã—160 â†’ 20Ã—20\n",
    "    px = (xx * 20 / H).astype(int)\n",
    "    py = (yy * 20 / W).astype(int)\n",
    "    patch = px + py * 20\n",
    "\n",
    "    ids = fiber_indices[patch]\n",
    "\n",
    "    names = []\n",
    "    for k in ids:\n",
    "        name = tract_name[tract_list[k]]\n",
    "        if isinstance(name, bytes):\n",
    "            name = name.decode()\n",
    "        names.append(name)\n",
    "\n",
    "    return np.unique(names)\n",
    "\n",
    "# ===================== input folder: (ä½ å·²ç»ç”Ÿæˆçš„ hotspot npy) =====================\n",
    "attn_folder = pathlib.Path(f\"{root}/attns_merged_hotspot_P97\")\n",
    "\n",
    "# ===================== output folder =====================\n",
    "out_dir = pathlib.Path(f\"{root}/fibers_from_attn160_P97\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ===================== éå†æ‰€æœ‰ attn160 æ–‡ä»¶ =====================\n",
    "for npy_file in sorted(attn_folder.glob(f\"*attn160.npy\")):\n",
    "\n",
    "    print(\"Processing:\", npy_file.name)\n",
    "    attn160 = np.load(npy_file)\n",
    "\n",
    "    T = threshold_p97(attn160)\n",
    "    fibers = extract_fibers(attn160, T)\n",
    "\n",
    "    # è¾“å‡º txt æ–‡ä»¶\n",
    "    txt_path = out_dir / (npy_file.stem + \"_fibers.txt\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        f.write(f\"Threshold(P97): {T}\\n\")\n",
    "        f.write(\"Fibers:\\n\")\n",
    "        for name in fibers:\n",
    "            f.write(f\"  {name}\\n\")\n",
    "\n",
    "print(\"ğŸ‰ Done! æ‰€æœ‰ attn160.npy çš„ fiber åç§°å·²ç»å…¨éƒ¨è¾“å‡º\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():   \n",
    "    model.eval()\n",
    "    preds = model(allimages)\n",
    "mixed = [((labels==0)&(preds==labels)).float().sum(),\n",
    "                 ((labels==0)&(preds!=labels)).float().sum(),\n",
    "                 ((labels==1)&(preds==labels)).float().sum(),\n",
    "                 ((labels==1)&(preds!=labels)).float().sum()]\n",
    "prec = mixed[0]/(mixed[0]+mixed[1])\n",
    "recall = mixed[0]/(mixed[0]+mixed[3])\n",
    "f1 = 2*prec*recall/(prec+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():   \n",
    "    model.eval()\n",
    "    preds = model(allimages)\n",
    "mixed = [((labels==0)&(preds==labels)).float().sum(),\n",
    "                 ((labels==0)&(preds!=labels)).float().sum(),\n",
    "                 ((labels==1)&(preds==labels)).float().sum(),\n",
    "                 ((labels==1)&(preds!=labels)).float().sum()]\n",
    "prec = mixed[0]/(mixed[0]+mixed[1])\n",
    "recall = mixed[0]/(mixed[0]+mixed[3])\n",
    "f1 = 2*prec*recall/(prec+recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
